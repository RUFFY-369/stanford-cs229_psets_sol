{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Neural Networks: MNIST image classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>774</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59595</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59596</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59597</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59598</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59599</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59600 rows Ã— 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5    6    7    8    9    ...  774  775  776  \\\n",
       "0        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "1        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "2        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "3        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "4        0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "59595    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59596    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59597    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59598    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "59599    0    0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
       "\n",
       "       777  778  779  780  781  782  783  \n",
       "0        0    0    0    0    0    0    0  \n",
       "1        0    0    0    0    0    0    0  \n",
       "2        0    0    0    0    0    0    0  \n",
       "3        0    0    0    0    0    0    0  \n",
       "4        0    0    0    0    0    0    0  \n",
       "...    ...  ...  ...  ...  ...  ...  ...  \n",
       "59595    0    0    0    0    0    0    0  \n",
       "59596    0    0    0    0    0    0    0  \n",
       "59597    0    0    0    0    0    0    0  \n",
       "59598    0    0    0    0    0    0    0  \n",
       "59599    0    0    0    0    0    0    0  \n",
       "\n",
       "[59600 rows x 784 columns]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Taking a look at the dataset(*****can remove this cell*****)\n",
    "\n",
    "\n",
    "\n",
    "data_f=pd.read_csv(r'images_train.csv',header=None)\n",
    "data_l=pd.read_csv(r'labels_train.csv',header=None)\n",
    "x_train=data_f.iloc[:59600,:]\n",
    "x_dev=data_f.iloc[59600:60000,:]\n",
    "y_train=data_l.iloc[:59600,:]\n",
    "y_dev=data_l.iloc[59600:60000,:]\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_POOL_SIZE = 5\n",
    "CONVOLUTION_SIZE = 4\n",
    "CONVOLUTION_FILTERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_softmax(x):\n",
    "    \"\"\"\n",
    "    Compute softmax function for a single example.\n",
    "    The shape of the input is of size # num classes.\n",
    "\n",
    "    Important Note: You must be careful to avoid overflow for this function. Functions\n",
    "    like softmax have a tendency to overflow when very large numbers like e^10000 are computed.\n",
    "    You will know that your function is overflow resistent when it can handle input like:\n",
    "    np.array([[10000, 10010, 10]]) without issues.\n",
    "\n",
    "    Args:\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array containing the softmax results of shape  number_of_classes\n",
    "    \"\"\"\n",
    "    x = x - np.max(x,axis=0)\n",
    "    exp = np.exp(x)\n",
    "    s = exp / np.sum(exp,axis=0)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_softmax(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x.\n",
    "\n",
    "    grad_outputs is the gradient of the loss with respect to the outputs of the softmax.\n",
    "\n",
    "    Args:\n",
    "        x: A 1d numpy float array of shape number_of_classes\n",
    "        grad_outputs: A 1d numpy flaot array of shape number_of_classes \n",
    "\n",
    "    Returns:\n",
    "        A 1d numpy float array of the same shape as x with the derivative of the loss with respect to x\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    soft_x=forward_softmax(x)\n",
    "    grad_x=soft_x*grad_outputs-soft_x*(soft_x@grad_outputs)\n",
    "    return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu function for the input x.\n",
    "\n",
    "    Args:\n",
    "        x: A numpy float array\n",
    "\n",
    "    Returns:\n",
    "        A numpy float array containing the relu results\n",
    "    \"\"\"\n",
    "\n",
    "    x[x<=0] = 0\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_relu(x, grad_outputs):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to x\n",
    "\n",
    "    Args:\n",
    "        x: A numpy array of arbitrary shape containing the input.\n",
    "        grad_outputs: A numpy array of the same shape of x containing the gradient of the loss with respect\n",
    "            to the output of relu\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of the same shape as x containing the gradients with respect to x.\n",
    "    \"\"\"\n",
    "    grad_relu_x=(x>=0)*(grad_outputs)\n",
    "    return grad_relu_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_params():\n",
    "    \"\"\"\n",
    "    Compute the initial parameters for the neural network.\n",
    "\n",
    "    This function should return a dictionary mapping parameter names to numpy arrays containing\n",
    "    the initial values for those parameters.\n",
    "\n",
    "    There should be four parameters for this model:\n",
    "    W1 is the weight matrix for the convolutional layer\n",
    "    b1 is the bias vector for the convolutional layer\n",
    "    W2 is the weight matrix for the output layers\n",
    "    b2 is the bias vector for the output layer\n",
    "\n",
    "    Weight matrices should be initialized with values drawn from a random normal distribution.\n",
    "    The mean of that distribution should be 0.\n",
    "    The variance of that distribution should be 1/sqrt(n) where n is the number of neurons that \n",
    "    feed into an output for that layer.\n",
    "\n",
    "    Bias vectors should be initialized with zero.\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        A dict mapping parameter names to numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    size_after_convolution = 28 - CONVOLUTION_SIZE + 1\n",
    "    size_after_max_pooling = size_after_convolution // MAX_POOL_SIZE\n",
    "\n",
    "    num_hidden = size_after_max_pooling * size_after_max_pooling * CONVOLUTION_FILTERS\n",
    "\n",
    "    return {\n",
    "        'W1': np.random.normal(size = (CONVOLUTION_FILTERS, 1, CONVOLUTION_SIZE, CONVOLUTION_SIZE), scale=1/ math.sqrt(CONVOLUTION_SIZE * CONVOLUTION_SIZE)),\n",
    "        'b1': np.zeros(CONVOLUTION_FILTERS),\n",
    "        'W2': np.random.normal(size = (num_hidden, 10), scale = 1/ math.sqrt(num_hidden)),\n",
    "        'b2': np.zeros(10)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_convolution(conv_W, conv_b, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a convolutional layer given the weights and data.\n",
    "\n",
    "    conv_W is of the shape (# output channels, # input channels, convolution width, convolution height )\n",
    "    conv_b is of the shape (# output channels)\n",
    "\n",
    "    data is of the shape (# input channels, width, height)\n",
    "\n",
    "    The output should be the result of a convolution and should be of the size:\n",
    "        (# output channels, width - convolution width + 1, height -  convolution height + 1)\n",
    "\n",
    "    Returns:\n",
    "        The output of the convolution as a numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((conv_channels, input_width - conv_width + 1, input_height - conv_height + 1))\n",
    "\n",
    "    for x in range(input_width - conv_width + 1):\n",
    "        for y in range(input_height - conv_height + 1):\n",
    "            for output_channel in range(conv_channels):\n",
    "                output[output_channel, x, y] = np.sum(\n",
    "                    np.multiply(data[:, x:(x + conv_width), y:(y + conv_height)], conv_W[output_channel, :, :, :])) + conv_b[output_channel]\n",
    "\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_convolution(conv_W, conv_b, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the parameters of the convolution.\n",
    "\n",
    "    See forward_convolution for the sizes of the arguments.\n",
    "    output_grad is the gradient of the loss with respect to the output of the convolution.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing 3 gradients.\n",
    "        The first element is the gradient of the loss with respect to the convolution weights\n",
    "        The second element is the gradient of the loss with respect to the convolution bias\n",
    "        The third element is the gradient of the loss with respect to the input data\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_channels, _, conv_width, conv_height = conv_W.shape\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "    \n",
    "    _, output_width,output_height = output_grad.shape\n",
    "\n",
    "    #initialisation\n",
    "    convw_grad=np.zeros_like(conv_W)\n",
    "    data_grad=np.zeros_like(data)\n",
    "    \n",
    "    convb_grad=np.sum(output_grad,axis=(1,2))\n",
    "    \n",
    "    \n",
    "    #iterating over the output units\n",
    "    for cvc in range(conv_channels):\n",
    "        for ow in range(output_width):\n",
    "            for oh in range(output_height):\n",
    "                convw_grad[cvc] += data[:, ow:ow+conv_width, oh:oh+conv_height] * output_grad[cvc, ow, oh]\n",
    "                data_grad[:, ow:ow+conv_width, oh:oh+conv_height] += conv_W[cvc] * output_grad[cvc, ow, oh]\n",
    "                \n",
    "    \n",
    "    return convw_grad,convb_grad,data_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_max_pool(data, pool_width, pool_height):\n",
    "    \"\"\"\n",
    "    Compute the output from a max pooling layer given the data and pool dimensions.\n",
    "\n",
    "    The stride length should be equal to the pool size\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "\n",
    "    The output should be the result of the max pooling layer and should be of size:\n",
    "        (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    Returns:\n",
    "        The result of the max pooling layer\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    input_channels, input_width, input_height = data.shape\n",
    "\n",
    "    output = np.zeros((input_channels, input_width // pool_width, input_height // pool_height))\n",
    "\n",
    "    for x in range(0, input_width, pool_width):\n",
    "        for y in range(0, input_height, pool_height):\n",
    "\n",
    "            output[:, x // pool_width, y // pool_height] = np.amax(data[:, x:(x + pool_width), y:(y + pool_height)], axis=(1, 2))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_max_pool(data, pool_width, pool_height, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the loss with respect to the data in the max pooling layer.\n",
    "\n",
    "    data is of the shape (# channels, width, height)\n",
    "    output_grad is of shape (# channels, width // pool_width, height // pool_height)\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of the backward max\n",
    "    pool layer.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the data (of same shape as data)\n",
    "    \"\"\"\n",
    "    \n",
    "    grad_data=np.zeros_like(data)\n",
    "    chans,output_width,output_height=output_grad.shape\n",
    "    \n",
    "    #iterating through output units\n",
    "    for c in range(chans):\n",
    "        for ow in range(output_width):\n",
    "            for oh in range(output_height):\n",
    "                in_w = ow* pool_width\n",
    "                in_h = oh* pool_height\n",
    "                # maximium element's index\n",
    "                ind = np.argmax(data[c, in_w:in_w+pool_width, in_h:in_h+pool_height])\n",
    "                grad_data[c, in_w:in_w+pool_width, in_h:in_h+pool_height].flat[ind] += output_grad[c, ow, oh]    #.flat[] for collapsing into one dimension\n",
    "    \n",
    "    \n",
    "    return grad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the output from a cross entropy loss layer given the probabilities and labels.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be a scalar\n",
    "\n",
    "    Returns:\n",
    "        The result of the log loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    result = 0\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            result += -np.log(probabilities[i])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_cross_entropy_loss(probabilities, labels):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cross entropy loss with respect to the probabilities.\n",
    "\n",
    "    probabilities is of the shape (# classes)\n",
    "    labels is of the shape (# classes)\n",
    "\n",
    "    The output should be the gradient with respect to the probabilities.\n",
    "\n",
    "    Returns:\n",
    "        The gradient of the loss with respect to the probabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    grad_prob=-labels/probabilities\n",
    "    \n",
    "    return grad_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_linear(weights, bias, data):\n",
    "    \"\"\"\n",
    "    Compute the output from a linear layer with the given weights, bias and data.\n",
    "    weights is of the shape (input # features, output # features)\n",
    "    bias is of the shape (output # features)\n",
    "    data is of the shape (input # features)\n",
    "\n",
    "    The output should be of the shape (output # features)\n",
    "\n",
    "    Returns:\n",
    "        The result of the linear layer\n",
    "    \"\"\"\n",
    "    return data.dot(weights) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_linear(weights, bias, data, output_grad):\n",
    "    \"\"\"\n",
    "    Compute the gradients of the loss with respect to the parameters of a linear layer.\n",
    "\n",
    "    See forward_linear for information about the shapes of the variables.\n",
    "\n",
    "    output_grad is the gradient of the loss with respect to the output of this layer.\n",
    "\n",
    "    This should return a tuple with three elements:\n",
    "    - The gradient of the loss with respect to the weights\n",
    "    - The gradient of the loss with respect to the bias\n",
    "    - The gradient of the loss with respect to the data\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    grad_w=np.outer(data, output_grad)  #outer product\n",
    "    grad_bias=output_grad\n",
    "    grad_data = output_grad@weights.T \n",
    "    \n",
    "    \n",
    "    return grad_w,grad_bias,grad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the forward layer given the data, labels, and params.\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input (shape is 1 by 28 by 28)\n",
    "        labels: A 1d numpy array containing the labels (shape is 10)\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2 and b2\n",
    "            W1 and b1 represent the weights and bias for the hidden layer of the network\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A 2 element tuple containing:\n",
    "            1. A numpy array The output (after the softmax) of the output layer\n",
    "            2. The average loss for these data elements\n",
    "    \"\"\"\n",
    "\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    W2 = params['W2']\n",
    "    b2 = params['b2']\n",
    "\n",
    "    first_convolution = forward_convolution(W1, b1, data)\n",
    "    first_max_pool = forward_max_pool(first_convolution, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(first_max_pool)\n",
    "\n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "   \n",
    "    logits = forward_linear(W2, b2, flattened)\n",
    "\n",
    "    y = forward_softmax(logits)\n",
    "    cost = forward_cross_entropy_loss(y, labels)\n",
    "\n",
    "    return y, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(data, labels, params):\n",
    "    \"\"\"\n",
    "    Implement the backward propegation gradient computation step for a neural network\n",
    "    \n",
    "    Args:\n",
    "        data: A numpy array containing the input for a single example\n",
    "        labels: A 1d numpy array containing the labels for a single example\n",
    "        params: A dictionary mapping parameter names to numpy arrays with the parameters.\n",
    "            This numpy array will contain W1, b1, W2, and b2\n",
    "            W1 and b1 represent the weights and bias for the convolutional layer\n",
    "            W2 and b2 represent the weights and bias for the output layer of the network\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of strings to numpy arrays where each key represents the name of a weight\n",
    "        and the values represent the gradient of the loss with respect to that weight.\n",
    "        \n",
    "        In particular, it should have 4 elements:\n",
    "            W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    conv_first=forward_convolution(params['W1'], params['b1'], data)\n",
    "    max_pool_first=forward_max_pool(conv_first, MAX_POOL_SIZE, MAX_POOL_SIZE)\n",
    "    first_after_relu = forward_relu(max_pool_first)\n",
    "    \n",
    "    flattened = np.reshape(first_after_relu, (-1))\n",
    "    logits = forward_linear(params['W2'], params['b2'], flattened)\n",
    "\n",
    "    y = forward_softmax(logits)\n",
    "    cost = forward_cross_entropy_loss(y, labels)\n",
    "    \n",
    "    \n",
    "    grad_y = backward_cross_entropy_loss(y, labels)\n",
    "    grad_logits = backward_softmax(logits, grad_y)\n",
    "    \n",
    "    \n",
    "    grad_W2, grad_b2, grad_flattend = backward_linear(params['W2'], params['b2'], flattened, grad_logits)\n",
    "    \n",
    "    grad_first_after_relu = grad_flattend.reshape(first_after_relu.shape)\n",
    "    \n",
    "    grad_max_pool_first = backward_relu(max_pool_first, grad_first_after_relu)\n",
    "    \n",
    "    grad_conv_first = backward_max_pool(conv_first, MAX_POOL_SIZE, MAX_POOL_SIZE, grad_max_pool_first)\n",
    "    \n",
    "    grad_W1, grad_b1, grad_data = backward_convolution(params['W1'], params['b1'], data, grad_conv_first)\n",
    "    \n",
    "    \n",
    "    grad={'W1': grad_W1, 'b1': grad_b1, 'W2': grad_W2, 'b2': grad_b2}\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop_batch(batch_data, batch_labels, params, forward_prop_func):\n",
    "    \"\"\"Apply the forward prop func to every image in a batch\"\"\"\n",
    "\n",
    "    y_array = []\n",
    "    cost_array = []\n",
    "\n",
    "    for item, label in zip(batch_data, batch_labels):\n",
    "        y, cost = forward_prop_func(item, label, params)\n",
    "        y_array.append(y)\n",
    "        cost_array.append(cost)\n",
    "\n",
    "    return np.array(y_array), np.array(cost_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_batch(batch_data, batch_labels, learning_rate, params, backward_prop_func):\n",
    "    \"\"\"\n",
    "    Perform one batch of gradient descent on the given training data using the provided learning rate.\n",
    "\n",
    "    This code should update the parameters stored in params.\n",
    "    It should not return anything\n",
    "\n",
    "    Args:\n",
    "        batch_data: A numpy array containing the training data for the batch\n",
    "        train_labels: A numpy array containing the training labels for the batch\n",
    "        learning_rate: The learning rate\n",
    "        params: A dict of parameter names to parameter values that should be updated.\n",
    "        backward_prop_func: A function that follows the backwards_prop API\n",
    "\n",
    "    Returns: This function returns nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    total_grad = {}\n",
    "\n",
    "    for i in range(batch_data.shape[0]):\n",
    "        grad = backward_prop_func(\n",
    "            batch_data[i, :, :], \n",
    "            batch_labels[i, :], \n",
    "            params)\n",
    "        for key, value in grad.items():\n",
    "            if key not in total_grad:\n",
    "                total_grad[key] = np.zeros(value.shape)\n",
    "\n",
    "            total_grad[key] += value\n",
    "\n",
    "    params['W1'] = params['W1'] - learning_rate * total_grad['W1']\n",
    "    params['W2'] = params['W2'] - learning_rate * total_grad['W2']\n",
    "    params['b1'] = params['b1'] - learning_rate * total_grad['b1']\n",
    "    params['b2'] = params['b2'] - learning_rate * total_grad['b2']\n",
    "\n",
    "    # This function does not return anything\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train(\n",
    "    train_data, train_labels, dev_data, dev_labels, \n",
    "    get_initial_params_func, forward_prop_func, backward_prop_func,\n",
    "    learning_rate=5, batch_size=16, num_batches=400):\n",
    "\n",
    "    m = train_data.shape[0]\n",
    "\n",
    "    params = get_initial_params_func()\n",
    "\n",
    "    cost_dev = []\n",
    "    accuracy_dev = []\n",
    "    for batch in range(num_batches):\n",
    "        print('Currently processing {} / {}'.format(batch, num_batches))\n",
    "\n",
    "        batch_data = train_data[batch * batch_size:(batch + 1) * batch_size, :, :, :]\n",
    "        batch_labels = train_labels[batch * batch_size: (batch + 1) * batch_size, :]\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            output, cost = forward_prop_batch(dev_data, dev_labels, params, forward_prop_func)\n",
    "            cost_dev.append(sum(cost) / len(cost))\n",
    "            accuracy_dev.append(compute_accuracy(output, dev_labels))\n",
    "\n",
    "            print('Cost and accuracy', cost_dev[-1], accuracy_dev[-1])\n",
    "\n",
    "        gradient_descent_batch(batch_data, batch_labels, \n",
    "            learning_rate, params, backward_prop_func)\n",
    "\n",
    "    return params, cost_dev, accuracy_dev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_test(data, labels, params):\n",
    "    output, cost = forward_pr(data, labels, params)\n",
    "    accuracy = compute_accuracy(output, labels)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(output, labels):\n",
    "    correct_output = np.argmax(output,axis=1)\n",
    "    correct_labels = np.argmax(labels,axis=1)\n",
    "\n",
    "    is_correct = [a == b for a,b in zip(correct_output, correct_labels)]\n",
    "\n",
    "    accuracy = sum(is_correct) * 1. / labels.shape[0]\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_labels(labels):\n",
    "    one_hot_labels = np.zeros((labels.size, 10))\n",
    "    one_hot_labels[np.arange(labels.size),labels.astype(int)] = 1\n",
    "    return one_hot_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(images_file, labels_file):\n",
    "    x = np.loadtxt(images_file, delimiter=',')\n",
    "    y = np.loadtxt(labels_file, delimiter=',')\n",
    "\n",
    "    x = np.reshape(x, (x.shape[0], 1, 28, 28))\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(all_data, all_labels, backward_prop_func):\n",
    "    params, cost_dev, accuracy_dev = nn_train(\n",
    "        all_data['train'], all_labels['train'], \n",
    "        all_data['dev'], all_labels['dev'],\n",
    "        get_initial_params, forward_prop, backward_prop_func,\n",
    "        learning_rate=1e-2, batch_size=16, num_batches=400\n",
    "    )\n",
    "\n",
    "    t = np.arange(400 // 100)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "    ax1.plot(t, cost_dev, 'b')\n",
    "    ax1.set_xlabel('time')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_title('Training curve')\n",
    "\n",
    "    ax2.plot(t, accuracy_dev, 'b')\n",
    "    ax2.set_xlabel('time')\n",
    "    ax2.set_ylabel('accuracy')\n",
    "\n",
    "    fig.savefig('output/train.pdf')\n",
    "    \n",
    "    print(f'The accuracy and cost on dev set is {accuracy_dev[-1]*100}%, {cost_dev[-1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing 0 / 400\n",
      "Cost and accuracy 2.721417647426753 0.0725\n",
      "Currently processing 1 / 400\n",
      "Currently processing 2 / 400\n",
      "Currently processing 3 / 400\n",
      "Currently processing 4 / 400\n",
      "Currently processing 5 / 400\n",
      "Currently processing 6 / 400\n",
      "Currently processing 7 / 400\n",
      "Currently processing 8 / 400\n",
      "Currently processing 9 / 400\n",
      "Currently processing 10 / 400\n",
      "Currently processing 11 / 400\n",
      "Currently processing 12 / 400\n",
      "Currently processing 13 / 400\n",
      "Currently processing 14 / 400\n",
      "Currently processing 15 / 400\n",
      "Currently processing 16 / 400\n",
      "Currently processing 17 / 400\n",
      "Currently processing 18 / 400\n",
      "Currently processing 19 / 400\n",
      "Currently processing 20 / 400\n",
      "Currently processing 21 / 400\n",
      "Currently processing 22 / 400\n",
      "Currently processing 23 / 400\n",
      "Currently processing 24 / 400\n",
      "Currently processing 25 / 400\n",
      "Currently processing 26 / 400\n",
      "Currently processing 27 / 400\n",
      "Currently processing 28 / 400\n",
      "Currently processing 29 / 400\n",
      "Currently processing 30 / 400\n",
      "Currently processing 31 / 400\n",
      "Currently processing 32 / 400\n",
      "Currently processing 33 / 400\n",
      "Currently processing 34 / 400\n",
      "Currently processing 35 / 400\n",
      "Currently processing 36 / 400\n",
      "Currently processing 37 / 400\n",
      "Currently processing 38 / 400\n",
      "Currently processing 39 / 400\n",
      "Currently processing 40 / 400\n",
      "Currently processing 41 / 400\n",
      "Currently processing 42 / 400\n",
      "Currently processing 43 / 400\n",
      "Currently processing 44 / 400\n",
      "Currently processing 45 / 400\n",
      "Currently processing 46 / 400\n",
      "Currently processing 47 / 400\n",
      "Currently processing 48 / 400\n",
      "Currently processing 49 / 400\n",
      "Currently processing 50 / 400\n",
      "Currently processing 51 / 400\n",
      "Currently processing 52 / 400\n",
      "Currently processing 53 / 400\n",
      "Currently processing 54 / 400\n",
      "Currently processing 55 / 400\n",
      "Currently processing 56 / 400\n",
      "Currently processing 57 / 400\n",
      "Currently processing 58 / 400\n",
      "Currently processing 59 / 400\n",
      "Currently processing 60 / 400\n",
      "Currently processing 61 / 400\n",
      "Currently processing 62 / 400\n",
      "Currently processing 63 / 400\n",
      "Currently processing 64 / 400\n",
      "Currently processing 65 / 400\n",
      "Currently processing 66 / 400\n",
      "Currently processing 67 / 400\n",
      "Currently processing 68 / 400\n",
      "Currently processing 69 / 400\n",
      "Currently processing 70 / 400\n",
      "Currently processing 71 / 400\n",
      "Currently processing 72 / 400\n",
      "Currently processing 73 / 400\n",
      "Currently processing 74 / 400\n",
      "Currently processing 75 / 400\n",
      "Currently processing 76 / 400\n",
      "Currently processing 77 / 400\n",
      "Currently processing 78 / 400\n",
      "Currently processing 79 / 400\n",
      "Currently processing 80 / 400\n",
      "Currently processing 81 / 400\n",
      "Currently processing 82 / 400\n",
      "Currently processing 83 / 400\n",
      "Currently processing 84 / 400\n",
      "Currently processing 85 / 400\n",
      "Currently processing 86 / 400\n",
      "Currently processing 87 / 400\n",
      "Currently processing 88 / 400\n",
      "Currently processing 89 / 400\n",
      "Currently processing 90 / 400\n",
      "Currently processing 91 / 400\n",
      "Currently processing 92 / 400\n",
      "Currently processing 93 / 400\n",
      "Currently processing 94 / 400\n",
      "Currently processing 95 / 400\n",
      "Currently processing 96 / 400\n",
      "Currently processing 97 / 400\n",
      "Currently processing 98 / 400\n",
      "Currently processing 99 / 400\n",
      "Currently processing 100 / 400\n",
      "Cost and accuracy 0.570011149598304 0.8275\n",
      "Currently processing 101 / 400\n",
      "Currently processing 102 / 400\n",
      "Currently processing 103 / 400\n",
      "Currently processing 104 / 400\n",
      "Currently processing 105 / 400\n",
      "Currently processing 106 / 400\n",
      "Currently processing 107 / 400\n",
      "Currently processing 108 / 400\n",
      "Currently processing 109 / 400\n",
      "Currently processing 110 / 400\n",
      "Currently processing 111 / 400\n",
      "Currently processing 112 / 400\n",
      "Currently processing 113 / 400\n",
      "Currently processing 114 / 400\n",
      "Currently processing 115 / 400\n",
      "Currently processing 116 / 400\n",
      "Currently processing 117 / 400\n",
      "Currently processing 118 / 400\n",
      "Currently processing 119 / 400\n",
      "Currently processing 120 / 400\n",
      "Currently processing 121 / 400\n",
      "Currently processing 122 / 400\n",
      "Currently processing 123 / 400\n",
      "Currently processing 124 / 400\n",
      "Currently processing 125 / 400\n",
      "Currently processing 126 / 400\n",
      "Currently processing 127 / 400\n",
      "Currently processing 128 / 400\n",
      "Currently processing 129 / 400\n",
      "Currently processing 130 / 400\n",
      "Currently processing 131 / 400\n",
      "Currently processing 132 / 400\n",
      "Currently processing 133 / 400\n",
      "Currently processing 134 / 400\n",
      "Currently processing 135 / 400\n",
      "Currently processing 136 / 400\n",
      "Currently processing 137 / 400\n",
      "Currently processing 138 / 400\n",
      "Currently processing 139 / 400\n",
      "Currently processing 140 / 400\n",
      "Currently processing 141 / 400\n",
      "Currently processing 142 / 400\n",
      "Currently processing 143 / 400\n",
      "Currently processing 144 / 400\n",
      "Currently processing 145 / 400\n",
      "Currently processing 146 / 400\n",
      "Currently processing 147 / 400\n",
      "Currently processing 148 / 400\n",
      "Currently processing 149 / 400\n",
      "Currently processing 150 / 400\n",
      "Currently processing 151 / 400\n",
      "Currently processing 152 / 400\n",
      "Currently processing 153 / 400\n",
      "Currently processing 154 / 400\n",
      "Currently processing 155 / 400\n",
      "Currently processing 156 / 400\n",
      "Currently processing 157 / 400\n",
      "Currently processing 158 / 400\n",
      "Currently processing 159 / 400\n",
      "Currently processing 160 / 400\n",
      "Currently processing 161 / 400\n",
      "Currently processing 162 / 400\n",
      "Currently processing 163 / 400\n",
      "Currently processing 164 / 400\n",
      "Currently processing 165 / 400\n",
      "Currently processing 166 / 400\n",
      "Currently processing 167 / 400\n",
      "Currently processing 168 / 400\n",
      "Currently processing 169 / 400\n",
      "Currently processing 170 / 400\n",
      "Currently processing 171 / 400\n",
      "Currently processing 172 / 400\n",
      "Currently processing 173 / 400\n",
      "Currently processing 174 / 400\n",
      "Currently processing 175 / 400\n",
      "Currently processing 176 / 400\n",
      "Currently processing 177 / 400\n",
      "Currently processing 178 / 400\n",
      "Currently processing 179 / 400\n",
      "Currently processing 180 / 400\n",
      "Currently processing 181 / 400\n",
      "Currently processing 182 / 400\n",
      "Currently processing 183 / 400\n",
      "Currently processing 184 / 400\n",
      "Currently processing 185 / 400\n",
      "Currently processing 186 / 400\n",
      "Currently processing 187 / 400\n",
      "Currently processing 188 / 400\n",
      "Currently processing 189 / 400\n",
      "Currently processing 190 / 400\n",
      "Currently processing 191 / 400\n",
      "Currently processing 192 / 400\n",
      "Currently processing 193 / 400\n",
      "Currently processing 194 / 400\n",
      "Currently processing 195 / 400\n",
      "Currently processing 196 / 400\n",
      "Currently processing 197 / 400\n",
      "Currently processing 198 / 400\n",
      "Currently processing 199 / 400\n",
      "Currently processing 200 / 400\n",
      "Cost and accuracy 0.3820302098337725 0.8675\n",
      "Currently processing 201 / 400\n",
      "Currently processing 202 / 400\n",
      "Currently processing 203 / 400\n",
      "Currently processing 204 / 400\n",
      "Currently processing 205 / 400\n",
      "Currently processing 206 / 400\n",
      "Currently processing 207 / 400\n",
      "Currently processing 208 / 400\n",
      "Currently processing 209 / 400\n",
      "Currently processing 210 / 400\n",
      "Currently processing 211 / 400\n",
      "Currently processing 212 / 400\n",
      "Currently processing 213 / 400\n",
      "Currently processing 214 / 400\n",
      "Currently processing 215 / 400\n",
      "Currently processing 216 / 400\n",
      "Currently processing 217 / 400\n",
      "Currently processing 218 / 400\n",
      "Currently processing 219 / 400\n",
      "Currently processing 220 / 400\n",
      "Currently processing 221 / 400\n",
      "Currently processing 222 / 400\n",
      "Currently processing 223 / 400\n",
      "Currently processing 224 / 400\n",
      "Currently processing 225 / 400\n",
      "Currently processing 226 / 400\n",
      "Currently processing 227 / 400\n",
      "Currently processing 228 / 400\n",
      "Currently processing 229 / 400\n",
      "Currently processing 230 / 400\n",
      "Currently processing 231 / 400\n",
      "Currently processing 232 / 400\n",
      "Currently processing 233 / 400\n",
      "Currently processing 234 / 400\n",
      "Currently processing 235 / 400\n",
      "Currently processing 236 / 400\n",
      "Currently processing 237 / 400\n",
      "Currently processing 238 / 400\n",
      "Currently processing 239 / 400\n",
      "Currently processing 240 / 400\n",
      "Currently processing 241 / 400\n",
      "Currently processing 242 / 400\n",
      "Currently processing 243 / 400\n",
      "Currently processing 244 / 400\n",
      "Currently processing 245 / 400\n",
      "Currently processing 246 / 400\n",
      "Currently processing 247 / 400\n",
      "Currently processing 248 / 400\n",
      "Currently processing 249 / 400\n",
      "Currently processing 250 / 400\n",
      "Currently processing 251 / 400\n",
      "Currently processing 252 / 400\n",
      "Currently processing 253 / 400\n",
      "Currently processing 254 / 400\n",
      "Currently processing 255 / 400\n",
      "Currently processing 256 / 400\n",
      "Currently processing 257 / 400\n",
      "Currently processing 258 / 400\n",
      "Currently processing 259 / 400\n",
      "Currently processing 260 / 400\n",
      "Currently processing 261 / 400\n",
      "Currently processing 262 / 400\n",
      "Currently processing 263 / 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently processing 264 / 400\n",
      "Currently processing 265 / 400\n",
      "Currently processing 266 / 400\n",
      "Currently processing 267 / 400\n",
      "Currently processing 268 / 400\n",
      "Currently processing 269 / 400\n",
      "Currently processing 270 / 400\n",
      "Currently processing 271 / 400\n",
      "Currently processing 272 / 400\n",
      "Currently processing 273 / 400\n",
      "Currently processing 274 / 400\n",
      "Currently processing 275 / 400\n",
      "Currently processing 276 / 400\n",
      "Currently processing 277 / 400\n",
      "Currently processing 278 / 400\n",
      "Currently processing 279 / 400\n",
      "Currently processing 280 / 400\n",
      "Currently processing 281 / 400\n",
      "Currently processing 282 / 400\n",
      "Currently processing 283 / 400\n",
      "Currently processing 284 / 400\n",
      "Currently processing 285 / 400\n",
      "Currently processing 286 / 400\n",
      "Currently processing 287 / 400\n",
      "Currently processing 288 / 400\n",
      "Currently processing 289 / 400\n",
      "Currently processing 290 / 400\n",
      "Currently processing 291 / 400\n",
      "Currently processing 292 / 400\n",
      "Currently processing 293 / 400\n",
      "Currently processing 294 / 400\n",
      "Currently processing 295 / 400\n",
      "Currently processing 296 / 400\n",
      "Currently processing 297 / 400\n",
      "Currently processing 298 / 400\n",
      "Currently processing 299 / 400\n",
      "Currently processing 300 / 400\n",
      "Cost and accuracy 0.3652316357919391 0.8925\n",
      "Currently processing 301 / 400\n",
      "Currently processing 302 / 400\n",
      "Currently processing 303 / 400\n",
      "Currently processing 304 / 400\n",
      "Currently processing 305 / 400\n",
      "Currently processing 306 / 400\n",
      "Currently processing 307 / 400\n",
      "Currently processing 308 / 400\n",
      "Currently processing 309 / 400\n",
      "Currently processing 310 / 400\n",
      "Currently processing 311 / 400\n",
      "Currently processing 312 / 400\n",
      "Currently processing 313 / 400\n",
      "Currently processing 314 / 400\n",
      "Currently processing 315 / 400\n",
      "Currently processing 316 / 400\n",
      "Currently processing 317 / 400\n",
      "Currently processing 318 / 400\n",
      "Currently processing 319 / 400\n",
      "Currently processing 320 / 400\n",
      "Currently processing 321 / 400\n",
      "Currently processing 322 / 400\n",
      "Currently processing 323 / 400\n",
      "Currently processing 324 / 400\n",
      "Currently processing 325 / 400\n",
      "Currently processing 326 / 400\n",
      "Currently processing 327 / 400\n",
      "Currently processing 328 / 400\n",
      "Currently processing 329 / 400\n",
      "Currently processing 330 / 400\n",
      "Currently processing 331 / 400\n",
      "Currently processing 332 / 400\n",
      "Currently processing 333 / 400\n",
      "Currently processing 334 / 400\n",
      "Currently processing 335 / 400\n",
      "Currently processing 336 / 400\n",
      "Currently processing 337 / 400\n",
      "Currently processing 338 / 400\n",
      "Currently processing 339 / 400\n",
      "Currently processing 340 / 400\n",
      "Currently processing 341 / 400\n",
      "Currently processing 342 / 400\n",
      "Currently processing 343 / 400\n",
      "Currently processing 344 / 400\n",
      "Currently processing 345 / 400\n",
      "Currently processing 346 / 400\n",
      "Currently processing 347 / 400\n",
      "Currently processing 348 / 400\n",
      "Currently processing 349 / 400\n",
      "Currently processing 350 / 400\n",
      "Currently processing 351 / 400\n",
      "Currently processing 352 / 400\n",
      "Currently processing 353 / 400\n",
      "Currently processing 354 / 400\n",
      "Currently processing 355 / 400\n",
      "Currently processing 356 / 400\n",
      "Currently processing 357 / 400\n",
      "Currently processing 358 / 400\n",
      "Currently processing 359 / 400\n",
      "Currently processing 360 / 400\n",
      "Currently processing 361 / 400\n",
      "Currently processing 362 / 400\n",
      "Currently processing 363 / 400\n",
      "Currently processing 364 / 400\n",
      "Currently processing 365 / 400\n",
      "Currently processing 366 / 400\n",
      "Currently processing 367 / 400\n",
      "Currently processing 368 / 400\n",
      "Currently processing 369 / 400\n",
      "Currently processing 370 / 400\n",
      "Currently processing 371 / 400\n",
      "Currently processing 372 / 400\n",
      "Currently processing 373 / 400\n",
      "Currently processing 374 / 400\n",
      "Currently processing 375 / 400\n",
      "Currently processing 376 / 400\n",
      "Currently processing 377 / 400\n",
      "Currently processing 378 / 400\n",
      "Currently processing 379 / 400\n",
      "Currently processing 380 / 400\n",
      "Currently processing 381 / 400\n",
      "Currently processing 382 / 400\n",
      "Currently processing 383 / 400\n",
      "Currently processing 384 / 400\n",
      "Currently processing 385 / 400\n",
      "Currently processing 386 / 400\n",
      "Currently processing 387 / 400\n",
      "Currently processing 388 / 400\n",
      "Currently processing 389 / 400\n",
      "Currently processing 390 / 400\n",
      "Currently processing 391 / 400\n",
      "Currently processing 392 / 400\n",
      "Currently processing 393 / 400\n",
      "Currently processing 394 / 400\n",
      "Currently processing 395 / 400\n",
      "Currently processing 396 / 400\n",
      "Currently processing 397 / 400\n",
      "Currently processing 398 / 400\n",
      "Currently processing 399 / 400\n",
      "The accuracy and cost on dev set is 89.25%, 0.3652316357919391\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    np.random.seed(100)\n",
    "    train_data, train_labels = read_data('../data/images_train.csv', '../data/labels_train.csv')\n",
    "    train_labels = one_hot_labels(train_labels)\n",
    "    p = np.random.permutation(60000)\n",
    "    train_data = train_data[p,:]\n",
    "    train_labels = train_labels[p,:]\n",
    "\n",
    "    dev_data = train_data[0:400,:]\n",
    "    dev_labels = train_labels[0:400,:]\n",
    "    train_data = train_data[400:,:]\n",
    "    train_labels = train_labels[400:,:]\n",
    "\n",
    "    mean = np.mean(train_data)\n",
    "    std = np.std(train_data)\n",
    "    train_data = (train_data - mean) / std\n",
    "    dev_data = (dev_data - mean) / std\n",
    "\n",
    "    all_data = {\n",
    "        'train': train_data,\n",
    "        'dev': dev_data,\n",
    "    }\n",
    "\n",
    "    all_labels = {\n",
    "        'train': train_labels,\n",
    "        'dev': dev_labels,\n",
    "    }\n",
    "    \n",
    "    run_train(all_data, all_labels, backward_prop)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
